# @package _global_

defaults:
  - hydra: default.yaml
  - plugins: deepspeed.yaml
  - logger: wandb.yaml
  # - logger: csv.yaml
  # - logger: tensorboard.yaml
  - hparam_search: null
  - dist_settings: null
  - trainer: default.yaml
  # override config either in this file or using experiment config
  - _self_
  - experiment: null # experiment configs allow for version control of specific hyperparameters

# Pipeline options
train: false
fine_tune: false
distill: false
validate: false
test: true
score: false
predict: false

# Setting
task: backward_prediction # ["forward_prediction", "backward_prediction", "mol_opt"]]
run_name: ${task} # change if you want a custom run name for the model checkpoint
project_name: ${run_name}
track: true
max_test_time: null

# Paths
home: /home/kvvb168/chem_reruns
output_directory: ${home}/tb_logs
deepspeed_config_path: ${home}/ds_config.json
track_path: ${output_directory}/emissions/${project_name}_${logger.name}_emissions.csv
output_score_data: ${hydra.runtime.output_dir}/metrics_scores.csv # Path to .csv file to which model score results should be written.
output_sampled_smiles: ${hydra.runtime.output_dir}/sampled_smiles.json # Path to .json file to which sampled smiles should be written.

# Trainer
seed: 73
resume: false
batch_size: 128
n_epochs: 50
limit_val_batches: 1.0
n_buckets: 24
n_gpus: 1
n_nodes: 1
acc_batches: 8
accelerator: null
check_val_every_n_epoch: 1

# trainer:
#   limit_test_batches: 1
#   # limit_val_batches: 1

# Data
data_path: null
dataset_type: synthesis
# "The datamodule to to use, example 'uspto_50'
# (see molbart.util.build_seq2seq_datamodule")
vocabulary_path: vocab/bart_vocab_downstream.json
augmentation_probability: 0.0
augmentation_strategy: all # Can be set to "all", "reactants", "products" when using synthesis datamodule

# Model
model_path: null
model_type: bart # ["bart", "unified"]
learning_rate: 0.001
weight_decay: 0.0
clip_grad: 1.0
d_model: 512
n_layers: 6
n_encoder_layers: ${n_layers}
n_decoder_layers: ${n_layers}
n_heads: 8
d_feedforward: 2048
train_tokens: null
max_seq_len: 512

moe: false
perceiver: false
share_weights: false
project_dim: null

batch_first: false

schedule: cycle
warm_up_steps: 8000

# Inference
n_beams: 10
n_unique_beams: null

n_chunks: 1
i_chunk: 0
dataset_part: test # Which dataset split to run inference on. [full", "train", "val", "test"]

# Distilled Model Overrides
student_d_model: ${d_model}
student_n_layers: ${n_layers}
student_n_encoder_layers: ${n_encoder_layers}
student_n_decoder_layers: ${n_decoder_layers}
student_n_heads: ${n_heads}
student_d_feedforward: ${d_feedforward}

# Callbacks
callbacks:
  - LearningRateMonitor
  - ModelCheckpoint:
      - every_n_epochs: 1
      - monitor: validation_loss
      - save_last: true
      - save_top_k: 0
  - ValidationScoreCallback
  - ScoreCallback
  # - StepCheckpoint
  - OptLRMonitor

scorers:
  - FractionInvalidScore
  - FractionUniqueScore
  - TanimotoSimilarityScore:
      - statistics: mean
  - TopKAccuracyScore
  - TopKValidAccuracyScore
  - TopKValidUniqueAccuracyScore
